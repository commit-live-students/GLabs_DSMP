{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsML</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "      <td>AaronPressman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "      <td>AaronPressman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "      <td>AaronPressman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "      <td>AaronPressman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "      <td>AaronPressman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              NewsML         Labels\n",
       "0  The Internet may be overflowing with new techn...  AaronPressman\n",
       "1  The U.S. Postal Service announced Wednesday a ...  AaronPressman\n",
       "2  Elementary school students with access to the ...  AaronPressman\n",
       "3  An influential Internet organisation has backe...  AaronPressman\n",
       "4  An influential Internet organisation has backe...  AaronPressman"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data for mini-challenges\n",
    "#Data contains text messages, we would use it to build a News classifier.\n",
    "df = pd.read_csv('../data/NewsMl.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Internet may be overflowing with new technology but crime in cyberspace is still of the old-fashioned variety. The National Consumers League said Wednesday that the most popular scam on the Internet was the pyramid scheme, in which early investors in a bogus fund are paid off with deposits of later investors. The league, a non-profit consumer advocacy group, tracks web scams through a site it set up on the world wide web in February called Internet Fraud Watch at http://www.fraud.org. The site, which collects reports directly from consumers, has been widely praised by law enforcement agencies. \"Consumers who suspect a scam on the Internet have critical information,\" said Jodie Bernstein, director of the Federal Trade Commission's Bureau of Consumer Protection. Internet Fraud Watch \"has been a major help to the FTC in identifying particular scams in their infancy.\" In May, for example, the commission used Internet reports to shut down a site run by Fortuna Alliance that had taken in over $6 million, promising investors they could earn $5,000 a month from an initial deposit of $250. Instead, Fortuna kept most of the money, the commission charged. Fraud reports from the league's site, which has been visited over 370,000 times, are forwarded to local, state and federal authorities. The second-most-popular Internet scam, the league said, was the sale of bogus Internet services, such as custom designed web sites or Internet access accounts. In third place were crooks who sell computer equipment, such as memory chips or sound boards, over the net and then deliver significantly lower quality goods or nothing at all, the league said. Other top scams involve business opportunities. Con artists may offer shares in a business or franchise using unreasonable predictions or misrepresentations. One popular scheme promised to let consumers get rich while working at home. The League also announced Tuesday that NationsBank had donated $100,000 to become a sponsor of the Fraud Watch site. \n",
      "============================================================\n",
      "AaronPressman\n"
     ]
    }
   ],
   "source": [
    "#Feature vectors\n",
    "X = df['NewsML']\n",
    "print(X[0])\n",
    "print(\"==\"*30)\n",
    "#target variable\n",
    "y = df['Labels'] \n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 1\n",
    "***\n",
    "### Instructions\n",
    "- Preprocess text data by doing the following steps on the `NewsML` column\n",
    "        - retain only alphabets\n",
    "        - Convert the data to lowercase and tokezize\n",
    "        - Remove stop words by using list comprehension\n",
    "        - join list elements\n",
    "- Finally split into train and test using train_test_split function where feature is `X`, target is `y`,test size is 20% and random state is 3. Save the resultant variables as X_train, X_test, y_train and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              NewsML         Labels\n",
      "0  internet may overflowing new technology crime ...  AaronPressman\n",
      "1  u postal service announced wednesday plan boos...  AaronPressman\n",
      "2  elementary school students access internet lea...  AaronPressman\n",
      "3  influential internet organisation backed away ...  AaronPressman\n",
      "4  influential internet organisation backed away ...  AaronPressman\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# stopwords \n",
    "stop = set(stopwords.words('english'))\n",
    "# retain only alphabets\n",
    "df[\"NewsML\"]=df[\"NewsML\"].apply(lambda x:re.sub(\"[^a-zA-Z]\", \" \",x))\n",
    "\n",
    "# convert to lowercase and tokenize\n",
    "df[\"NewsML\"]=df[\"NewsML\"].apply(lambda x:x.lower().split())\n",
    "\n",
    "# remove stopwords\n",
    "df[\"NewsML\"]=df[\"NewsML\"].apply(lambda x:[i for i in x if i not in stop])\n",
    "\n",
    "# join list elements\n",
    "df[\"NewsML\"]=df[\"NewsML\"].apply(lambda x: ' '.join(x))\n",
    "print(df.head())\n",
    "\n",
    "# split into training and test sets\n",
    "X_train, X_test, y_train , y_test=train_test_split(X, y, test_size = 0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 2\n",
    "***\n",
    "**Vectorize with Bag-of-words and TF-IDF approach**: <br>\n",
    "After cleaning data its time to vectorize data so that it can be fed into an ML algorithm. You will be doing it with two approaches: Bag-of-words and TF-IDF.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Initialize Bag-of-words vectorizer using CountVectorizer() and TF-IDF vectorizer using TfidfVectorizer(ngram_range=(1,3)).     Save them as count_vectorizer and tfidf_vectorizer respectively.\n",
    "\n",
    "- Next thing to do is fit each vectorizer on training and test features with text data and transform them to vectors.\n",
    "\n",
    "- First fit and transform data with count_vectorizer on X_train using .fit_transform(X_train) method of count_vectorizer and     save it as `X_train_count`\n",
    "\n",
    "- Use this fitted version of count_vectorizer on X_test and transform X_test with .transform(X_test) method of                   count_vectorizer. Save it as `X_test_count`\n",
    "\n",
    "- Similarly repeat the previous two steps with tfidf_vectorizer and save the transformed training feature as `X_train_tfidf`     and transformed test feature as `X_test_tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "# initialize count vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# initialize tfidf vectorizer\n",
    "tfidf_vectorizer= TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "\n",
    "# fit and transform with count vectorizer\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count =count_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# fit and transform with tfidf vectorizer\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf =tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 3\n",
    "***\n",
    "**Predicting with Multinomial Naive Bayes**:\n",
    "Multinomial Naive Bayes is an algorithm that can be used for the purpose of multi-class classification. You will be using it to train and test it on both the versions i.e. Bag-of-words and TF-IDF ones and then checking the accuracy on both of them.\n",
    "\n",
    "### Instructions\n",
    "- Initialize two Multinomial Naive Bayes classifiers with MultinomialNB() and save them as `nb_1` and `nb_2`. The reason for initializing two classifiers is because you will be training and testing on both Bag-of-words and TF-IDF transformed training data.\n",
    "\n",
    "- Fit nb1 on X_train_count and y_train using `.fit()` method\n",
    "\n",
    "- Fit nb2 on X_train_tfidf and y_train using `.fit()` method\n",
    "\n",
    "- Find the accuracy with Bag-of-words approach using accuracy_score(nb_1.predict(X_test_count), y_test) and save it as           `acc_count_nb \n",
    "\n",
    "- Similarly find the accuracy for the TF-IDF approach (only difference is the classifer is nb_2) and save it as `acc_tfidf_nb`\n",
    "\n",
    "- Print out `acc_count_nb` and `acc_tfidf_nb` to check which version performs better for with Multinomial Naive Bayes as         classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Count Vectorizer:  0.92\n",
      "Accuracy tfidf Vectorizer:  0.95\n",
      "Wall time: 76.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# initialize multinomial naive bayes\n",
    "nb_1 = MultinomialNB()\n",
    "nb_2 = MultinomialNB()\n",
    "\n",
    "# fit on count vectorizer training data\n",
    "nb_1.fit(X_train_count,y_train)\n",
    "\n",
    "# fit on tfidf vectorizer training data\n",
    "nb_2.fit(X_train_tfidf,y_train)\n",
    "\n",
    "# accuracy with count vectorizer\n",
    "acc_count_nb = accuracy_score(nb_1.predict(X_test_count), y_test)\n",
    "\n",
    "# accuracy with tfidf vectorizer\n",
    "acc_tfidf_nb = accuracy_score(nb_2.predict(X_test_tfidf), y_test)\n",
    "\n",
    "# display accuracies\n",
    "print(\"Accuracy Count Vectorizer: \",acc_count_nb)\n",
    "print(\"Accuracy tfidf Vectorizer: \",acc_tfidf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:- We achieved 95% accuracy with tfidf vectorizer and 92% with count vectorizer by using MultinomialNB algorithm. Lets see if we can increase the accuracy by using Logistic Regression on both vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 4\n",
    "***\n",
    "**Predicting with Logistic Regression**\n",
    "Logistic Regression can be used for binary classification but when combined with OneVsRest classifer, it can perform multiclass classification as well. You will be using one such algorithm to train and test it on both the versions i.e. Bag-of-words and TF-IDF ones and then checking the accuracy on both of them\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- First initialize two classifiers with OneVsRestClassifier(LogisticRegression(random_state=10)) and save them as `logreg_1`     and `logreg_2`. The reason for initializing two classifiers is because you will be training and testing on both Bag-of-words   and TF-IDF transformed training data.\n",
    "\n",
    "- Fit logreg_1 on X_train_count and Y_train using .fit() method\n",
    "\n",
    "- Fit logreg_2 on X_train_tfidf and Y_train using .fit() method\n",
    "\n",
    "- Find the accuracy with Bag-of-words approach using accuracy_score(logreg_1.predict(X_test_count), Y_test) and save it as       `acc_count_logreg`\n",
    "\n",
    "- Similarly find the accuracy for the TF-IDF approach (only difference is the classifer is logreg_2) and save it as               `acc_tfidf_logreg`\n",
    "\n",
    "- Print out `acc_count_logreg` and `acc_tfidf_logreg` to check which version performs better for with Multinomial Naive Bayes     as classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for count:  0.96\n",
      "Accuracy for tfidf:  0.95\n",
      "Wall time: 750 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# initialize logistic regression\n",
    "logreg_1 = OneVsRestClassifier(LogisticRegression(random_state=10)) \n",
    "logreg_2 = OneVsRestClassifier(LogisticRegression(random_state=10)) \n",
    "\n",
    "# fit on count vectorizer training data\n",
    "logreg_1.fit(X_train_count , y_train) \n",
    "\n",
    "# fit on tfidf vectorizer training data\n",
    "logreg_2.fit(X_train_tfidf , y_train) \n",
    "\n",
    "# accuracy with count vectorizer\n",
    "acc_count_logreg=accuracy_score(logreg_1.predict(X_test_count), y_test) \n",
    "\n",
    "# accuracy with tfidf vectorizer\n",
    "acc_tfidf_logreg =accuracy_score(logreg_2.predict(X_test_tfidf), y_test) \n",
    "\n",
    "# display accuracies\n",
    "print(\"Accuracy for count: \",acc_count_logreg)\n",
    "print(\"Accuracy for tfidf: \",acc_tfidf_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: We can see that for Count Vectorizer the accuracy is increased i.e 96% and for tfidf vectorizer the accuracy is same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/quiz.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Foundation of Text Analytics\n",
    "***\n",
    "Q1. N-grams are defined as the combination of N keywords together. How many bi-grams can be generated from given sentence:\n",
    "“Greyatom is a great source to learn data science”\n",
    "```python\n",
    "A) 7\n",
    "B) 8\n",
    "C) 9\n",
    "D) 10\n",
    "E) 11\n",
    "\n",
    "Solution: (B)\n",
    "Greyatom is,is a, a great, great source, source to, To learn, learn data, data science\n",
    "```\n",
    "Q2. Which of the following models can perform tweet classification with regards to context mentioned above?\n",
    " \n",
    " Suppose You have collected a data of about 10,000 rows of tweet text and no other information. You want to create a tweet      classification model that categorizes each of the tweets in three buckets – positive, negative and neutral.\n",
    "```python\n",
    "A) Naive Bayes\n",
    "B) SVM\n",
    "C) None of the above\n",
    "\n",
    "Solution: (C)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
